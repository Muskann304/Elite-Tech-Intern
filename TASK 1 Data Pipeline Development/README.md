# ETL Pipeline for Titanic Dataset

This project demonstrates a simple ETL (Extract, Transform, Load) pipeline using Python and scikit-learn on the Titanic dataset from OpenML.

## ğŸ”§ Features

- âœ… Data extraction from OpenML
- ğŸ” Data cleaning and preprocessing
- ğŸ“Š One-hot encoding for categorical features
- ğŸ“ˆ Scaling for numeric features
- ğŸ’¾ Final data export to CSV

## ğŸ“ File Structure

```
etl_pipeline.py          # Main ETL pipeline script
requirements.txt         # Python dependencies
README.md                # Project overview
processed_data.csv       # Output (generated after running the pipeline)
```

## â–¶ï¸ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/data-science-etl.git
   cd data-science-etl
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the ETL pipeline:
   ```bash
   python etl_pipeline.py
   ```

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).
